{
  "metadata": {
    "kernelspec": {
      "display_name": "Python37 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    },
    "lastEditStatus": {
      "notebookId": "rs3422hvtbxnhpyo5epa",
      "authorId": "5095547476787",
      "authorName": "EBOTWICK",
      "authorEmail": "elliott.botwick@snowflake.com",
      "sessionId": "3d75a8ec-7071-447f-969c-5ea1ee7c29be",
      "lastEditTime": 1763408536778
    }
  },
  "nbformat_minor": 4,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "id": "e79ae8e5-aec2-4276-9443-074c3a614142",
      "metadata": {
        "collapsed": false,
        "name": "INTRO_MD",
        "codeCollapsed": true
      },
      "source": "# ❄️ End-to-end ML Demo ❄️\n\nIn this workflow we will work through the following elements of a typical tabular machine learning pipeline.\n\n### 1. Use Feature Store to track engineered features\n* Store feature definitions in feature store for reproducible computation of ML features\n      \n### 2. Train two Models using the Snowflake ML APIs\n* Baseline XGboost\n* XGboost with optimal hyper-parameters identified via Snowflake ML distributed HPO methods\n\n### 3. Register both models in Snowflake model registry\n* Explore model registry capabilities such as **metadata tracking, inference, and explainability**\n* Compare model metrics on train/test set to identify any issues of model performance or overfitting\n* Tag the best performing model version as 'default' version\n### 4. Set up Model Monitor to track 1 year of predicted and actual patient readmissions\n* **Compute performance metrics** such as F1, Precision, Recall\n* **Inspect model drift** (i.e. how much has the average predicted readmission rate changed day-to-day)\n* **Compare models** side-by-side to understand which model should be used in production\n* Identify and understand **data issues**\n\n### 5. Track data and model lineage throughout\n* View and understand\n  * The **origin of the data** used for computed features\n  * The **data used** for model training\n  * The **available model versions** being monitored"
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "a2512cb5-15ae-40b2-84c7-8a44a9979670",
      "metadata": {
        "language": "python",
        "name": "pip_installs",
        "collapsed": true,
        "codeCollapsed": true
      },
      "outputs": [],
      "source": "! pip install snowflake-ml-python==1.20.0\nimport ray\nruntime_env = {\n    \"pip\": [\"snowflake-ml-python==1.20.0\"]\n}\nray.init(runtime_env=runtime_env)\n\n"
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "d78265b8-8baa-4136-a32a-32f3f620949d",
      "metadata": {
        "codeCollapsed": false,
        "collapsed": false,
        "language": "python",
        "name": "set_version_num_and_vars"
      },
      "outputs": [],
      "source": "#Update this VERSION_NUM to version your features, models etc!\nVERSION_NUM = '0'\nDB = \"E2E_SNOW_MLOPS_DB\" \nSCHEMA = \"MLOPS_SCHEMA\" \nCOMPUTE_WAREHOUSE = \"E2E_SNOW_MLOPS_WH\" "
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "ce110000-1111-2222-3333-ffffff000000",
      "metadata": {
        "codeCollapsed": true,
        "collapsed": true,
        "language": "python",
        "name": "imports_and_session",
        "resultHeight": 84
      },
      "outputs": [],
      "source": "import pandas as pd\nimport numpy as np\nimport sklearn\nimport math\nimport pickle\nfrom datetime import datetime\nfrom xgboost import XGBClassifier\n\n\n\n# Snowpark ML\nfrom snowflake.ml.registry import Registry\nfrom snowflake.ml.modeling.tune import get_tuner_context\nfrom snowflake.ml.modeling import tune\nfrom entities import search_algorithm\n\n#Snowflake feature store\nfrom snowflake.ml.feature_store import FeatureStore, FeatureView, Entity, CreationMode\n\n# Snowpark session\nfrom snowflake.snowpark import DataFrame\nfrom snowflake.snowpark.functions import col, to_timestamp, min, max, month, dayofweek, dayofyear, avg, date_add, sql_expr\nfrom snowflake.snowpark.types import IntegerType\nfrom snowflake.snowpark import Window\n\n#setup snowpark session\nfrom snowflake.snowpark.context import get_active_session\nsession = get_active_session()\nsession.use_role(\"E2E_SNOW_MLOPS_ROLE\")\nsession.use_database(\"E2E_SNOW_MLOPS_DB\")\nsession.use_schema(\"MLOPS_SCHEMA\")\nsession.use_warehouse(\"E2E_SNOW_MLOPS_WH\")"
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "f8900d1d-a1f2-419b-ae7e-b194f268d904",
      "metadata": {
        "codeCollapsed": false,
        "collapsed": false,
        "language": "python",
        "name": "read_raw_data",
        "resultHeight": 223
      },
      "outputs": [],
      "source": "try:\n    print(\"Reading table data...\")\n    df = session.table(\"PATIENT_READMISSION_DEMO_DATA\")\n    df.show(5)\nexcept:\n    print(\"Table not found! Uploading data to snowflake table\")\n    df_pandas = pd.read_csv(\"PATIENT_READMISSION_DEMO_DATA.csv.zip\")\n    df_pandas.columns = [c.upper() for c in df_pandas.columns]\n    session.write_pandas(df_pandas, \"PATIENT_READMISSION_DEMO_DATA\", auto_create_table=True)\n    df = session.table(\"PATIENT_READMISSION_DEMO_DATA\")\n    df.show(5)"
    },
    {
      "cell_type": "markdown",
      "id": "60938b6f-bda7-4783-ae44-547bd34d98de",
      "metadata": {
        "collapsed": false,
        "name": "md1",
        "codeCollapsed": true
      },
      "source": "## Observe Snowflake Snowpark table properties"
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "a6654de7-6407-4ffe-a214-fd66078397ef",
      "metadata": {
        "codeCollapsed": false,
        "collapsed": false,
        "language": "python",
        "name": "see_timespan",
        "resultHeight": 111
      },
      "outputs": [],
      "source": "df.select(min('TS'), max('TS')).show()"
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "4b5a38cc-c479-4839-b0ae-9e5cb3e0facb",
      "metadata": {
        "codeCollapsed": false,
        "language": "python",
        "name": "find_timedelta"
      },
      "outputs": [],
      "source": "#Get current date and time\ncurrent_time = datetime.now()\ndf_max_time = datetime.strptime(str(df.select(max(\"TS\")).collect()[0][0]), \"%Y-%m-%d %H:%M:%S.%f\")\n\n#Find delta between latest existing timestamp and today's date\ntimedelta = current_time- df_max_time\n\n#Update timestamps to represent last ~1 year from today's date\ndf.select(min(date_add(to_timestamp(\"TS\"), timedelta.days-1)), max(date_add(to_timestamp(\"TS\"), timedelta.days-1)))"
    },
    {
      "cell_type": "markdown",
      "id": "8aa46c7d-519b-422c-8932-9b031fc6b4bd",
      "metadata": {
        "collapsed": false,
        "name": "feat_eng_md",
        "codeCollapsed": true
      },
      "source": "## Feature Engineering with Snowpark APIs"
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "b355c0c4-9dc6-4faf-86b7-24d8d559e453",
      "metadata": {
        "codeCollapsed": false,
        "collapsed": false,
        "language": "python",
        "name": "define_features",
        "resultHeight": 0
      },
      "outputs": [],
      "source": "#Create a dict with keys for feature names and values containing transform code\n\nfeature_eng_dict = dict()\n\n#Timestamp features\nfeature_eng_dict[\"TIMESTAMP\"] = date_add(to_timestamp(\"TS\"), timedelta.days-1)\nfeature_eng_dict[\"MONTH\"] = month(\"TIMESTAMP\")\nfeature_eng_dict[\"DAY_OF_YEAR\"] = dayofyear(\"TIMESTAMP\") \nfeature_eng_dict[\"DOTW\"] = dayofweek(\"TIMESTAMP\")\n\n# df= df.with_columns(feature_eng_dict.keys(), feature_eng_dict.values())\n\n#Medical and patient features\nfeature_eng_dict[\"TREATMENT_COST\"] = col(\"TREATMENT_COST_1000S\")*1000\nfeature_eng_dict[\"PATIENT_AGE\"] = col(\"PATIENT_AGE_YEARS\")\nfeature_eng_dict[\"LENGTH_OF_STAY\"] = col(\"LENGTH_OF_STAY_DAYS\")\nfeature_eng_dict[\"COST_PER_DAY\"] = col(\"TREATMENT_COST\")/col(\"LENGTH_OF_STAY\")\n\ndf = df.with_columns(feature_eng_dict.keys(), feature_eng_dict.values())\ndf.show()"
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "b6c4ead8-25ac-46cc-9bd9-17eac2f796d5",
      "metadata": {
        "codeCollapsed": false,
        "collapsed": false,
        "language": "python",
        "name": "df_explain",
        "resultHeight": 312
      },
      "outputs": [],
      "source": "df.explain()"
    },
    {
      "cell_type": "markdown",
      "id": "72d7645e-e0ac-4539-b132-54ce53431402",
      "metadata": {
        "collapsed": false,
        "name": "feature_store_markdown",
        "codeCollapsed": true
      },
      "source": "## Create a Snowflake Feature Store"
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "abacdc71-9f2c-419f-8d50-3e8f89be367f",
      "metadata": {
        "codeCollapsed": false,
        "collapsed": false,
        "language": "python",
        "name": "define_feature_store",
        "resultHeight": 0
      },
      "outputs": [],
      "source": "fs = FeatureStore(\n    session=session, \n    database=DB, \n    name=SCHEMA, \n    default_warehouse=COMPUTE_WAREHOUSE,\n    creation_mode=CreationMode.CREATE_IF_NOT_EXIST\n)"
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "67480d6a-183f-4373-aaa8-d3ed8e80e11d",
      "metadata": {
        "codeCollapsed": false,
        "collapsed": false,
        "language": "python",
        "name": "list_entities",
        "resultHeight": 111
      },
      "outputs": [],
      "source": "fs.list_entities()"
    },
    {
      "cell_type": "markdown",
      "id": "d915406f-e52d-4baf-9f6c-b9e0e8d53e6e",
      "metadata": {
        "collapsed": false,
        "name": "FS_CONFIG_MD",
        "codeCollapsed": true
      },
      "source": "## Feature Store configuration\n- create/register entities of interest"
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "e91d6d39-7819-4825-8729-a3f19ca5cdf7",
      "metadata": {
        "codeCollapsed": false,
        "collapsed": false,
        "language": "python",
        "name": "load_or_register_entity",
        "resultHeight": 38
      },
      "outputs": [],
      "source": "#First try to retrieve an existing entity definition, if not define a new one and register\ntry:\n    #retrieve existing entity\n    patient_id_entity = fs.get_entity('PATIENT_ENTITY') \n    print('Retrieved existing entity')\nexcept:\n#define new entity\n    patient_id_entity = Entity(\n        name = \"PATIENT_ENTITY\",\n        join_keys = [\"PATIENT_ID\"],\n        desc = \"Features defined on a per patient level\")\n    #register\n    fs.register_entity(patient_id_entity)\n    print(\"Registered new entity\")"
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "2820463f-0ea7-43ea-a500-9b034011887d",
      "metadata": {
        "codeCollapsed": false,
        "collapsed": false,
        "language": "python",
        "name": "create_feature_df",
        "resultHeight": 217
      },
      "outputs": [],
      "source": "#Create a dataframe with just the ID, timestamp, and engineered features. We will use this to define our feature view\nfeature_df = df.select([\"PATIENT_ID\"]+list(feature_eng_dict.keys()))\nfeature_df.show(5)"
    },
    {
      "cell_type": "markdown",
      "id": "5cf84fe3-4120-4092-b43d-8873da57d461",
      "metadata": {
        "collapsed": false,
        "name": "FS_MD",
        "codeCollapsed": true
      },
      "source": "Here, the feature store references an existing table. \n\nWe could also define the dataframe via the use of Snowpark APIs, and use that dataframe (or a function that returns a dataframe) as the feature view definition, below."
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "2b53364f-90c4-45b4-94ee-b2fde6f93475",
      "metadata": {
        "codeCollapsed": false,
        "collapsed": false,
        "language": "python",
        "name": "feature_veiw_creation",
        "resultHeight": 0
      },
      "outputs": [],
      "source": "#define and register feature view\npatient_fv = FeatureView(\n    name=\"Patient_Readmission_Feature_View\",\n    entities=[patient_id_entity],\n    feature_df=feature_df,\n    timestamp_col=\"TIMESTAMP\",\n    refresh_freq=\"1 day\")\n\n#add feature level descriptions\n\npatient_fv = patient_fv.attach_feature_desc(\n    {\n        \"MONTH\": \"Month of admission\",\n        \"DAY_OF_YEAR\": \"Day of calendar year of admission\",\n        \"DOTW\": \"Day of the week of admission\",\n        \"TREATMENT_COST\": \"Treatment cost in $USD\",\n        \"PATIENT_AGE\": \"Patient age in years\",\n        \"LENGTH_OF_STAY\": \"Length of hospital stay in days\",\n        \"COST_PER_DAY\": \"Treatment cost per day in $USD\",\n    }\n)\n\npatient_fv = fs.register_feature_view(patient_fv, version=VERSION_NUM, overwrite=True)"
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "18c3225b-b936-4aa7-81f2-27bbaeee1c0f",
      "metadata": {
        "codeCollapsed": false,
        "collapsed": false,
        "language": "python",
        "name": "show_feature_views",
        "resultHeight": 111,
        "title": "show_feature_views"
      },
      "outputs": [],
      "source": "fs.list_feature_views()"
    },
    {
      "cell_type": "markdown",
      "id": "e96ff67f-bb04-40cb-8c14-11b5ebb2917d",
      "metadata": {
        "collapsed": false,
        "name": "FV_MD",
        "codeCollapsed": true
      },
      "source": "## Retrieve a Dataset from the featureview\n\nSnowflake Datasets are immutable, file-based objects that exist within your Snowpark session. \n\nThey can be written to persistent Snowflake objects as needed. "
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "535efc80-e4fc-41c5-98eb-5b5450bcf199",
      "metadata": {
        "codeCollapsed": false,
        "collapsed": false,
        "language": "python",
        "name": "generate_dataset",
        "resultHeight": 0
      },
      "outputs": [],
      "source": "ds = fs.generate_dataset(\n    name=f\"PATIENT_DATASET_EXTENDED_FEATURES_{VERSION_NUM}\",\n    spine_df=df.select(\"PATIENT_ID\", \"TIMESTAMP\", \"ADMISSION_TYPE\",\"READMITTED\"), #only need the features used to fetch rest of feature view\n    features=[patient_fv],\n    spine_timestamp_col=\"TIMESTAMP\",\n    spine_label_cols=[\"READMITTED\"]\n)"
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "ecdaa537-3fb9-476c-9153-3236edfdfcb3",
      "metadata": {
        "codeCollapsed": false,
        "collapsed": false,
        "language": "python",
        "name": "convert_dataset_to_snowpark_and_pandas",
        "resultHeight": 239
      },
      "outputs": [],
      "source": "ds_sp = ds.read.to_snowpark_dataframe()\nds_sp.show(5)"
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "b5e17036-7a69-4915-b025-49c900aeb46b",
      "metadata": {
        "codeCollapsed": false,
        "collapsed": false,
        "language": "python",
        "name": "one_hot_encoding",
        "resultHeight": 360
      },
      "outputs": [],
      "source": "import snowflake.ml.modeling.preprocessing as snowml\nfrom snowflake.snowpark.types import StringType\n\nOHE_COLS = ds_sp.select([col.name for col in ds_sp.schema if col.datatype ==StringType()]).columns\nOHE_POST_COLS = [i+\"_OHE\" for i in OHE_COLS]\n\n\n# Encode categoricals to numeric columns\nsnowml_ohe = snowml.OneHotEncoder(input_cols=OHE_COLS, output_cols = OHE_COLS, drop_input_cols=True)\nds_sp_ohe = snowml_ohe.fit(ds_sp).transform(ds_sp)\n\nds_sp_ohe.columns"
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "d834f6f3-ce15-405e-8fec-1d1bb5c224a6",
      "metadata": {
        "codeCollapsed": false,
        "collapsed": false,
        "language": "python",
        "name": "train_test_split",
        "resultHeight": 0
      },
      "outputs": [],
      "source": "train, test = ds_sp_ohe.random_split(weights=[0.70, 0.30], seed=0)"
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "a8ff103e-5314-4e95-87ba-d784b1102f36",
      "metadata": {
        "codeCollapsed": false,
        "collapsed": false,
        "language": "python",
        "name": "fill_na",
        "resultHeight": 0
      },
      "outputs": [],
      "source": "train = train.fillna(0)\ntest = test.fillna(0)"
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "c917df7f-e277-4fbb-abf5-1a4433367e3b",
      "metadata": {
        "codeCollapsed": false,
        "collapsed": false,
        "language": "python",
        "name": "convert_data_to_pandas"
      },
      "outputs": [],
      "source": "train_pd = train.to_pandas()\ntest_pd = test.to_pandas()"
    },
    {
      "cell_type": "markdown",
      "id": "38c05dc9-2efb-4c5f-995a-486ef926c6c5",
      "metadata": {
        "collapsed": false,
        "name": "model_training_md",
        "codeCollapsed": true
      },
      "source": "## Model Training\n### Below we will define and fit an xgboost classifier as our baseline model and evaluate the performance\n##### Note this is all done with OSS frameworks"
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "5e4b5fba-b7a8-47ff-aaf6-076b9e78dcaf",
      "metadata": {
        "codeCollapsed": false,
        "collapsed": false,
        "language": "python",
        "name": "define_model",
        "resultHeight": 0
      },
      "outputs": [],
      "source": "#Define model config\nxgb_base = XGBClassifier(\n    max_depth=50,\n    n_estimators=3,\n    learning_rate = 0.75,\n    booster = 'gbtree')"
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "644f3295-2496-4fd0-ae95-922a78c5b944",
      "metadata": {
        "codeCollapsed": false,
        "collapsed": false,
        "language": "python",
        "name": "train_base_model",
        "resultHeight": 1759
      },
      "outputs": [],
      "source": "#Split train data into X, y\nX_train_pd = train_pd.drop([\"TIMESTAMP\", \"PATIENT_ID\", \"READMITTED\"],axis=1) #remove\ny_train_pd = train_pd.READMITTED\n\n#train model\nxgb_base.fit(X_train_pd,y_train_pd)"
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "id": "0c5ac861-fcf9-47b2-9c11-ec44ee2367e4",
      "metadata": {
        "codeCollapsed": false,
        "collapsed": false,
        "language": "python",
        "name": "compute_predictions_and_perf_metrics"
      },
      "outputs": [],
      "source": "from sklearn.metrics import f1_score, precision_score, recall_score\ntrain_preds_base = xgb_base.predict(X_train_pd) #update this line with correct ata\n\nf1_base_train = round(f1_score(y_train_pd, train_preds_base),4)\nprecision_base_train = round(precision_score(y_train_pd, train_preds_base),4)\nrecall_base_train = round(recall_score(y_train_pd, train_preds_base),4)\n\nprint(f'F1: {f1_base_train} \\nPrecision {precision_base_train} \\nRecall: {recall_base_train}')"
    },
    {
      "cell_type": "markdown",
      "id": "93777778-d2ba-42d5-88c4-a90ba18c5006",
      "metadata": {
        "collapsed": false,
        "name": "model_regisry_md",
        "resultHeight": 74,
        "codeCollapsed": true
      },
      "source": "# Model Registry\n\n- Log models with important metadata\n- Manage model lifecycles\n- Serve models from Snowflake runtimes"
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "id": "21678e59-deaf-4c2b-b01e-1c59fe31b10a",
      "metadata": {
        "codeCollapsed": false,
        "collapsed": false,
        "language": "python",
        "name": "define_model_registry",
        "resultHeight": 0
      },
      "outputs": [],
      "source": "#Create a snowflake model registry object \nfrom snowflake.ml.registry import Registry\n\n# Define model name\nmodel_name = f\"PATIENT_READMISSION_MLOPS_{VERSION_NUM}\"\n\n# Create a registry to log the model to\nmodel_registry = Registry(session=session, \n                          database_name=DB, \n                          schema_name=SCHEMA,\n                          options={\"enable_monitoring\": True})"
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "id": "be41c3ac-49f0-4fd9-a557-9d8eb633f602",
      "metadata": {
        "codeCollapsed": false,
        "collapsed": false,
        "language": "python",
        "name": "register_model_version",
        "resultHeight": 229
      },
      "outputs": [],
      "source": "#Log the base model to the model registry (if not already there)\nbase_version_name = 'XGB_BASE'\n\ntry:\n    #Check for existing model\n    mv_base = model_registry.get_model(model_name).version(base_version_name)\n    print(\"Found existing model version!\")\nexcept:\n    print(\"Logging new model version...\")\n    #Log model to registry\n    mv_base = model_registry.log_model(\n        model_name=model_name,\n        model=xgb_base, \n        version_name=base_version_name,\n        sample_input_data = train.drop([\"TIMESTAMP\", \"PATIENT_ID\", \"READMITTED\"]).limit(100), #using snowpark df to maintain lineage\n        comment = f\"\"\"ML model for predicting patient readmission likelihood.\n                    This model was trained using XGBoost classifier.\n                    Hyperparameters used were:\n                    max_depth={xgb_base.max_depth}, \n                    n_estimators={xgb_base.n_estimators}, \n                    learning_rate = {xgb_base.learning_rate}, \n                    algorithm = {xgb_base.booster}\n                    \"\"\",\n        target_platforms= [\"WAREHOUSE\", \"SNOWPARK_CONTAINER_SERVICES\"],\n        options= {\"enable_explainability\": True}\n\n    )\n    \n    #set metrics\n    mv_base.set_metric(metric_name=\"Train_F1_Score\", value=f1_base_train)\n    mv_base.set_metric(metric_name=\"Train_Precision_Score\", value=precision_base_train)\n    mv_base.set_metric(metric_name=\"Train_Recall_score\", value=recall_base_train)"
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "id": "68e2ddab-b02a-4e05-8121-4e97e49e0eea",
      "metadata": {
        "codeCollapsed": false,
        "language": "python",
        "name": "create_prod_tag"
      },
      "outputs": [],
      "source": "#Create tag for PROD model\nsession.sql(\"CREATE OR REPLACE TAG PROD\").collect()"
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "id": "9e0054df-0cd9-4e81-98b8-6564be86b4b9",
      "metadata": {
        "codeCollapsed": false,
        "language": "python",
        "name": "create_PROD_tag"
      },
      "outputs": [],
      "source": "#Apply prod tag \nm = model_registry.get_model(model_name)\nm.comment = \"Loan approval prediction models\" #set model level comment\nm.set_tag(\"PROD\", base_version_name)\nm.show_tags()"
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "id": "ac4e294e-929d-4399-b2bb-d5d2d1dd043e",
      "metadata": {
        "codeCollapsed": false,
        "collapsed": false,
        "language": "python",
        "name": "show_models",
        "resultHeight": 111
      },
      "outputs": [],
      "source": "model_registry.show_models()"
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "id": "e3dfb281-9751-48a1-a76e-43ffffd9d099",
      "metadata": {
        "codeCollapsed": false,
        "collapsed": false,
        "language": "python",
        "name": "show_model_versions",
        "resultHeight": 146
      },
      "outputs": [],
      "source": "model_registry.get_model(model_name).show_versions()"
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "id": "eb1af8a1-7a92-455e-b9a1-8f2c699dfdeb",
      "metadata": {
        "codeCollapsed": false,
        "collapsed": false,
        "language": "python",
        "name": "print_model_version_and_metrics",
        "resultHeight": 239
      },
      "outputs": [],
      "source": "print(mv_base)\nprint(mv_base.show_metrics())"
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "id": "8ecdf05c-b3b5-4755-bdff-fd187ef07f58",
      "metadata": {
        "codeCollapsed": false,
        "collapsed": false,
        "language": "python",
        "name": "show_model_functions",
        "resultHeight": 2133
      },
      "outputs": [],
      "source": "mv_base.show_functions()"
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "id": "bf495261-a8a7-46be-b9c8-3f099268d154",
      "metadata": {
        "codeCollapsed": false,
        "collapsed": false,
        "language": "python",
        "name": "predict_from_registry",
        "resultHeight": 351
      },
      "outputs": [],
      "source": "reg_preds = mv_base.run(test, function_name = \"predict\").rename(col('\"output_feature_0\"'), \"READMISSION_PREDICTION\")\nreg_preds.show(10)"
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "id": "9ef61447-10e7-4a38-a429-3da3facf9ce7",
      "metadata": {
        "codeCollapsed": false,
        "collapsed": false,
        "language": "python",
        "name": "compute_test_metrics"
      },
      "outputs": [],
      "source": "#ds_sp_ohe = ds_sp_ohe.rename(col('\"LOAN_PURPOSE_NAME_Home improvement\"'), \"LOAN_PURPOSE_NAME_Home_improvement\")\n\npreds_pd = reg_preds.select([\"READMITTED\", \"READMISSION_PREDICTION\"]).to_pandas()\nf1_base_test = round(f1_score(preds_pd.READMITTED, preds_pd.READMISSION_PREDICTION),4)\nprecision_base_test = round(precision_score(preds_pd.READMITTED, preds_pd.READMISSION_PREDICTION),4)\nrecall_base_test = round(recall_score(preds_pd.READMITTED, preds_pd.READMISSION_PREDICTION),4)\n\n#log metrics to model registry model\nmv_base.set_metric(metric_name=\"Test_F1_Score\", value=f1_base_test)\nmv_base.set_metric(metric_name=\"Test_Precision_Score\", value=precision_base_test)\nmv_base.set_metric(metric_name=\"Test_Recall_score\", value=recall_base_test)\n\nprint(f'F1: {f1_base_test} \\nPrecision {precision_base_test} \\nRecall: {recall_base_test}')"
    },
    {
      "cell_type": "markdown",
      "id": "9b477885-35ce-486d-9e86-7d0cc9d48454",
      "metadata": {
        "collapsed": false,
        "name": "HPO_MD",
        "codeCollapsed": true
      },
      "source": "## Oh no! Our model's performance seems to have dropped off significantly from training to our test set. \n\n#### This is evidence that our model is overfit - can we fix this with Distributed Hyperparameter Optimization??"
    },
    {
      "cell_type": "markdown",
      "id": "60c8c205-afc2-46de-838d-495ad40d7de3",
      "metadata": {
        "name": "exp_tracking_md",
        "collapsed": false,
        "codeCollapsed": true
      },
      "source": "## This is also an excellent opportunity to test out Snowflake's Experiment Tracking Functionality \n\n#### Snowflake Experiment Tracking provides a mechanism for creating experiments and logging runs within Snowflake from any development environment. This capability allows you to log key pieces of information regarding your model training runs such as model parameters and metrics. In the UI, you can deep dive into a particular run or compare multiple runs to find the optimal model.\n\n#### Below we will train multiple models using distributed HPO and log results to the Experiment Tracker!"
    },
    {
      "cell_type": "code",
      "id": "e4d6860a-da49-42bc-aed9-57692eb5c7a2",
      "metadata": {
        "language": "python",
        "name": "define_HPO_with_exp_tracking"
      },
      "outputs": [],
      "source": "from snowflake.ml.data import DataConnector\nfrom snowflake.ml.modeling.tune import get_tuner_context\nfrom snowflake.ml.modeling import tune\nfrom entities import search_algorithm\nimport psutil\nfrom snowflake.ml.experiment.experiment_tracking import ExperimentTracking\n\n#Define dataset map\ndataset_map = {\n    \"x_train\": DataConnector.from_dataframe(train.drop(\"READMITTED\", \"TIMESTAMP\", \"PATIENT_ID\")),\n    \"y_train\": DataConnector.from_dataframe(train.select(\"READMITTED\")),\n    \"x_test\": DataConnector.from_dataframe(test.drop(\"READMITTED\",\"TIMESTAMP\", \"PATIENT_ID\")),\n    \"y_test\": DataConnector.from_dataframe(test.select(\"READMITTED\"))\n    }\n\n# exp = ExperimentTracking(session=session)\n\n# Define a training function, with any models you choose within it.\ndef train_func():\n\n    # A context object provided by HPO API to expose data for the current HPO trial\n    \n    tuner_context = get_tuner_context()\n    \n    #Generate params\n    config = tuner_context.get_hyper_params()\n    dm = tuner_context.get_dataset_map()\n    \n    #Instantiate mdoel with generated params\n    model = XGBClassifier(**config, random_state=42)\n\n    X_train_pd = dm[\"x_train\"].to_pandas().sort_index()\n    y_train_pd = dm[\"y_train\"].to_pandas().sort_index()\n    X_test_pd = dm[\"x_test\"].to_pandas().sort_index()\n    y_test_pd = dm[\"y_test\"].to_pandas().sort_index()\n\n    #Train model, get preds\n    model.fit(X_train_pd,y_train_pd)\n\n    #Run inference on train preds\n    train_preds = model.predict(X_train_pd)\n\n    #Run inference on test preds\n    test_preds = model.predict(X_test_pd)\n    \n    #compute metrics \n    f1_train = f1_score(y_train_pd,train_preds)\n    precision_train = precision_score(y_train_pd,train_preds)\n    recall_train = recall_score(y_train_pd,train_preds)\n\n    f1_test = f1_score(y_test_pd,test_preds)\n    precision_test = precision_score(y_test_pd,test_preds)\n    recall_test = recall_score(y_test_pd,test_preds)\n\n    metrics_to_log = {\"F1_Train\": f1_train,\n                     \"Precision_Train\": precision_train,\n                     \"Recall_Train\": recall_train,\n                     \"F1_Test\": f1_test,\n                     \"Precision_Test\": precision_test,\n                     \"Recall_Test\": recall_test,}\n\n    tuner_context.report(metrics=metrics_to_log, model=model)\n        \ntuner = tune.Tuner(\n    train_func=train_func,\n    search_space={\n        \"max_depth\": tune.randint(1, 30),\n        \"learning_rate\": tune.uniform(0.01, 0.5),\n        \"n_estimators\": tune.randint(50, 150),\n    },\n    tuner_config=tune.TunerConfig(\n        metric=\"F1_Test\",\n        mode=\"max\",\n        search_alg=search_algorithm.RandomSearch(random_state=101),\n        num_trials=8, #run 8 trial runs\n        max_concurrent_trials=1 # Use all available CPUs to run distributed HPO across. GPUs can also be used here! \n    ),\n)",
      "execution_count": 38
    },
    {
      "cell_type": "code",
      "id": "832807a0-2572-402f-9b7a-1803a3c91c19",
      "metadata": {
        "language": "python",
        "name": "run_hpo_and_inspect_results"
      },
      "outputs": [],
      "source": "tuner_results = tuner.run(dataset_map=dataset_map)\ntuner_results.results",
      "execution_count": 39
    },
    {
      "id": "da63c953-bc00-4ed8-ad0a-208a3eb3007d",
      "cell_type": "code",
      "metadata": {
        "language": "python",
        "name": "log_experiment",
        "title": "log_experiment"
      },
      "source": "exp = ExperimentTracking(session=session)\nexp.set_experiment(\"E2E_MLOPS_HPO_Experiments\")\n\ncols = tuner_results.results.columns\nmetrics = tuner_results.results[[c for c in cols if c.split(\"_\")[-1] in ['Train','Test']]].to_dict(orient='records')\nconfigs = tuner_results.results[[c for c in cols if c.startswith(\"config/\")]].to_dict(orient='records')\n\nfor cfg, mt in zip(configs,metrics):\n    with exp.start_run():\n        exp.log_params(cfg)\n        exp.log_metrics(mt)",
      "outputs": [],
      "execution_count": 50
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "id": "9ee37c42-3de7-476a-b7c0-d56952dac385",
      "metadata": {
        "codeCollapsed": false,
        "collapsed": false,
        "language": "python",
        "name": "inspect_hpo_params"
      },
      "outputs": [],
      "source": "#Select best model results and inspect configuration\ntuned_model = tuner_results.best_model\ntuned_model"
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "id": "94b4a6c2-674e-4d02-afdb-8ebf10cffdc4",
      "metadata": {
        "codeCollapsed": false,
        "collapsed": false,
        "language": "python",
        "name": "compute_hpo_train_predictions_and_metrics"
      },
      "outputs": [],
      "source": "#Generate predictions\nxgb_opt_preds = tuned_model.predict(train_pd.drop([\"TIMESTAMP\", \"PATIENT_ID\", \"READMITTED\"],axis=1))\n\n#Generate performance metrics\nf1_opt_train = round(f1_score(train_pd.READMITTED, xgb_opt_preds),4)\nprecision_opt_train = round(precision_score(train_pd.READMITTED, xgb_opt_preds),4)\nrecall_opt_train = round(recall_score(train_pd.READMITTED, xgb_opt_preds),4)\n\nprint(f'Train Results: \\nF1: {f1_opt_train} \\nPrecision {precision_opt_train} \\nRecall: {recall_opt_train}')"
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "id": "dee80c48-d521-4b77-8841-54ba35ecd4b6",
      "metadata": {
        "codeCollapsed": false,
        "collapsed": false,
        "language": "python",
        "name": "compute_hpo_test_predictions_and_metrics"
      },
      "outputs": [],
      "source": "#Generate test predictions\nxgb_opt_preds_test = tuned_model.predict(test_pd.drop([\"TIMESTAMP\", \"PATIENT_ID\", \"READMITTED\"],axis=1))\n\n#Generate performance metrics on test data\nf1_opt_test = round(f1_score(test_pd.READMITTED, xgb_opt_preds_test),4)\nprecision_opt_test = round(precision_score(test_pd.READMITTED, xgb_opt_preds_test),4)\nrecall_opt_test = round(recall_score(test_pd.READMITTED, xgb_opt_preds_test),4)\n\nprint(f'Test Results: \\nF1: {f1_opt_test} \\nPrecision {precision_opt_test} \\nRecall: {recall_opt_test}')"
    },
    {
      "cell_type": "markdown",
      "id": "89a1a670-52e3-4d77-ac3a-db830e22fdcf",
      "metadata": {
        "collapsed": false,
        "name": "HPO_performance_reaction",
        "codeCollapsed": true
      },
      "source": "## Here we see the HPO model has a more modest train accuracy than our base model - but the peformance doesn't drop off on new data (test set) "
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "id": "d501cf7d-4965-4b9f-8b16-edab897d0e18",
      "metadata": {
        "collapsed": false,
        "language": "python",
        "name": "log_hpo_model"
      },
      "outputs": [],
      "source": "#Log the optimized model to the model registry (if not already there)\noptimized_version_name = 'XGB_Optimized'\n\ntry:\n    #Check for existing model\n    mv_opt = model_registry.get_model(model_name).version(optimized_version_name)\n    print(\"Found existing model version!\")\nexcept:\n    #Log model to registry\n    print(\"Logging new model version...\")\n    mv_opt = model_registry.log_model(\n        model_name=model_name,\n        model=tuned_model, \n        version_name=optimized_version_name,\n        sample_input_data = train.drop([\"TIMESTAMP\", \"PATIENT_ID\", \"READMITTED\"]).limit(100),\n        comment = f\"\"\"HPO ML model for predicting patient readmission likelihood.\n            This model was trained using XGBoost classifier.\n            Optimized hyperparameters used were:\n            max_depth={tuned_model.max_depth}, \n            n_estimators={tuned_model.n_estimators}, \n            learning_rate = {tuned_model.learning_rate}, \n            \"\"\",\n        target_platforms= [\"WAREHOUSE\", \"SNOWPARK_CONTAINER_SERVICES\"],\n        options= {\"enable_explainability\": True}\n\n        \n\n    )\n    #Set metrics\n    mv_opt.set_metric(metric_name=\"Train_F1_Score\", value=f1_opt_train)\n    mv_opt.set_metric(metric_name=\"Train_Precision_Score\", value=precision_opt_train)\n    mv_opt.set_metric(metric_name=\"Train_Recall_score\", value=recall_opt_train)\n\n    mv_opt.set_metric(metric_name=\"Test_F1_Score\", value=f1_opt_test)\n    mv_opt.set_metric(metric_name=\"Test_Precision_Score\", value=precision_opt_test)\n    mv_opt.set_metric(metric_name=\"Test_Recall_score\", value=recall_opt_test)"
    },
    {
      "id": "7bf56b6e-fc59-46ed-a1af-4d5a76ab5258",
      "cell_type": "code",
      "metadata": {
        "language": "python"
      },
      "source": "model_name",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "id": "c4c028b9-b590-45b4-9884-35ee206bca0d",
      "metadata": {
        "codeCollapsed": false,
        "collapsed": false,
        "language": "python",
        "name": "inspect_current_default_version"
      },
      "outputs": [],
      "source": "#Here we see the BASE version is our default version\nmodel_registry.get_model(model_name).default"
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "id": "04ac97a9-7af4-4331-bb0d-cf6ecc4a77f6",
      "metadata": {
        "codeCollapsed": false,
        "collapsed": false,
        "language": "python",
        "name": "promote_optimized_version_to_default"
      },
      "outputs": [],
      "source": "#Now we'll set the optimized model to be the default model version going forward\nmodel_registry.get_model(model_name).default = optimized_version_name"
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "id": "c04efcee-27e6-4423-b669-849bec7cc8fb",
      "metadata": {
        "codeCollapsed": false,
        "collapsed": false,
        "language": "python",
        "name": "see_updated_model_versions"
      },
      "outputs": [],
      "source": "#Now we see our optimized version we have now recently promoted to our DEFAULT model version\nmodel_registry.get_model(model_name).default"
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "id": "8cc92f7f-5f02-4cc5-82d0-758f65f2d485",
      "metadata": {
        "codeCollapsed": false,
        "collapsed": false,
        "language": "python",
        "name": "update_model_tags"
      },
      "outputs": [],
      "source": "#we'll now update the PROD tagged model to be the optimized model version rather than our overfit base version\nm.unset_tag(\"PROD\")\nm.set_tag(\"PROD\", optimized_version_name)\nm.show_tags()"
    },
    {
      "cell_type": "markdown",
      "id": "05fff15e-5f49-4d4f-a02a-93e8f3114b11",
      "metadata": {
        "collapsed": false,
        "name": "explainability_MD",
        "codeCollapsed": true
      },
      "source": "\n## Now that we've deployed some model versions and tested inference... \n# Let's explain our models!\n- ### Snowflake offers built in explainability capabilities on top of models logged to the model registry\n- ### In the below section we'll generate shapley values using these built in functions to understand how input features impact our model's behavior"
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "id": "914f5cd6-d254-42d4-a0be-9848c9d09d4a",
      "metadata": {
        "collapsed": false,
        "language": "python",
        "name": "compute_shap_vals",
        "resultHeight": 0
      },
      "outputs": [],
      "source": "#create a sample of 1000 records\ntest_pd_sample=test_pd.rename(columns=rename_dict).sample(n=2500, random_state = 100).reset_index(drop=True)\n\n#Compute shapley values for each model\nbase_shap_pd = mv_base.run(test_pd_sample, function_name=\"explain\")\nopt_shap_pd = mv_opt.run(test_pd_sample, function_name=\"explain\")"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f74e0dcc-a850-474a-b475-f05a77619731",
      "metadata": {
        "codeCollapsed": false,
        "collapsed": false,
        "language": "python",
        "name": "base_shap_summary_plot",
        "resultHeight": 571
      },
      "outputs": [],
      "source": "import shap \n\nshap.summary_plot(np.array(base_shap_pd.astype(float)), \n                  test_pd_sample.drop([\"PATIENT_ID\",\"READMITTED\", \"TIMESTAMP\"], axis=1), \n                  feature_names = test_pd_sample.drop([\"PATIENT_ID\",\"READMITTED\", \"TIMESTAMP\"], axis=1).columns)"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "67469a84-3d44-49e4-8d6e-5cd8a6e8a633",
      "metadata": {
        "codeCollapsed": false,
        "language": "python",
        "name": "opt_shap_summary_plot"
      },
      "outputs": [],
      "source": "shap.summary_plot(np.array(opt_shap_pd.astype(float)), \n                  test_pd_sample.drop([\"PATIENT_ID\",\"READMITTED\", \"TIMESTAMP\"], axis=1), \n                  feature_names = test_pd_sample.drop([\"PATIENT_ID\",\"READMITTED\", \"TIMESTAMP\"], axis=1).columns)"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f3a0d4c3-750c-4ae0-9812-85b677db6986",
      "metadata": {
        "codeCollapsed": false,
        "collapsed": false,
        "language": "python",
        "name": "create_all_shap_dfs"
      },
      "outputs": [],
      "source": "#Merge shap vals and actual vals together for easier plotting below\nall_shap_base = test_pd_sample.merge(base_shap_pd, right_index=True, left_index=True, how='outer')\nall_shap_opt = test_pd_sample.merge(opt_shap_pd, right_index=True, left_index=True, how='outer')"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "938441fd-9ae3-4f97-9a54-b7e4c74738ac",
      "metadata": {
        "codeCollapsed": false,
        "language": "python",
        "name": "plot_patient_age_explanation",
        "title": "plot_patient_age_explanation"
      },
      "outputs": [],
      "source": "import seaborn as sns\nimport matplotlib.pyplot as plt\n\n#filter data down to strip outliers\nasb_filtered = all_shap_base[(all_shap_base.PATIENT_AGE>0) & (all_shap_base.PATIENT_AGE<100)]\naso_filtered = all_shap_opt[(all_shap_opt.PATIENT_AGE>0) & (all_shap_opt.PATIENT_AGE<100)]\n\n# Set up the figure\nfig, axes = plt.subplots(1, 2, figsize=(10, 6))\nfig.suptitle(\"PATIENT AGE EXPLANATION\")\n# Plot side-by-side boxplots\nsns.scatterplot(data = asb_filtered, x ='PATIENT_AGE', y = 'PATIENT_AGE_explanation', ax=axes[0])\nsns.regplot(data = asb_filtered, x =\"PATIENT_AGE\", y = 'PATIENT_AGE_explanation', scatter=False, color='red', line_kws={\"lw\":2},ci =100, lowess=False, ax =axes[0])\n\naxes[0].set_title('Base Model')\nsns.scatterplot(data = aso_filtered, x ='PATIENT_AGE', y = 'PATIENT_AGE_explanation',color = \"orange\", ax = axes[1])\nsns.regplot(data = aso_filtered, x =\"PATIENT_AGE\", y = 'PATIENT_AGE_explanation', scatter=False, color='blue', line_kws={\"lw\":2},ci =100, lowess=False, ax =axes[1])\naxes[1].set_title('Opt Model')\n\n# Customize and show the plot\nfor ax in axes:\n    ax.set_xlabel(\"Patient Age\")\n    ax.set_ylabel(\"Influence\")\nplt.tight_layout()\nplt.show()"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2298b1f8-0495-42e1-b668-0dcd03d8bb7c",
      "metadata": {
        "codeCollapsed": false,
        "collapsed": false,
        "language": "python",
        "name": "plot_treatment_cost_explanation",
        "title": "plot_treatment_cost_explanation"
      },
      "outputs": [],
      "source": "#filter data down to strip outliers\nasb_filtered = all_shap_base[all_shap_base.TREATMENT_COST<500000]\naso_filtered = all_shap_opt[all_shap_opt.TREATMENT_COST<500000]\n\n\n# Set up the figure\nfig, axes = plt.subplots(1, 2, figsize=(10, 6))\nfig.suptitle(\"TREATMENT_COST EXPLANATION\")\n# Plot side-by-side boxplots\nsns.scatterplot(data = asb_filtered, x ='TREATMENT_COST', y = 'TREATMENT_COST_explanation', ax=axes[0])\nsns.regplot(data = asb_filtered, x =\"TREATMENT_COST\", y = 'TREATMENT_COST_explanation', scatter=False, color='red', line_kws={\"lw\":2},ci =100, lowess=True, ax =axes[0])\naxes[0].set_title('Base Model')\n\nsns.scatterplot(data = aso_filtered, x ='TREATMENT_COST', y = 'TREATMENT_COST_explanation',color = \"orange\", ax = axes[1])\nsns.regplot(data = aso_filtered, x =\"TREATMENT_COST\", y = 'TREATMENT_COST_explanation', scatter=False, color='blue', line_kws={\"lw\":2},ci =100, lowess=True, ax =axes[1])\naxes[1].set_title('Opt Model')\n\n# Customize and show the plot\nfor ax in axes:\n    ax.set_xlabel(\"TREATMENT_COST\")\n    ax.set_ylabel(\"Influence\")\n    # ax.set_xlim((0,10000))\nplt.tight_layout()\nplt.show()"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "14a03aa9-1f1a-4a4e-809e-b22e438d72aa",
      "metadata": {
        "codeCollapsed": false,
        "collapsed": false,
        "language": "python",
        "name": "plot_emergency_admission_explanation",
        "resultHeight": 851,
        "title": "plot_emergency_admission_explanation"
      },
      "outputs": [],
      "source": "# Set up the figure\nfig, axes = plt.subplots(1, 2, figsize=(10, 6))\nfig.suptitle(\"EMERGENCY ADMISSION EXPLANATION\")\n# Plot side-by-side boxplots\nsns.boxplot(data = all_shap_base, x ='ADMISSION_TYPE_EMERGENCY', y = 'ADMISSION_TYPE_EMERGENCY_explanation',\n            hue='ADMISSION_TYPE_EMERGENCY', width=0.8, ax=axes[0])\naxes[0].set_title('Base Model')\nsns.boxplot(data = all_shap_opt, x ='ADMISSION_TYPE_EMERGENCY', y = 'ADMISSION_TYPE_EMERGENCY_explanation',\n            hue='ADMISSION_TYPE_EMERGENCY', width=0.4, ax = axes[1])\naxes[1].set_title('Opt Model')\n\n# Customize and show the plot\nfor ax in axes:\n    ax.set_xlabel(\"Emergency Admission (1 = True)\")\n    ax.set_ylabel(\"Influence\")\n    ax.legend(loc='upper right')\n\nplt.show()"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "66ea7aad-4e48-4666-a48c-ddc39331cb1f",
      "metadata": {
        "codeCollapsed": false,
        "collapsed": false,
        "language": "python",
        "name": "plot_elective_admission_explanation",
        "title": "plot_elective_admission_explanation"
      },
      "outputs": [],
      "source": "# Set up the figure\nfig, axes = plt.subplots(1, 2, figsize=(10, 6))\nfig.suptitle(\"ELECTIVE ADMISSION EXPLANATION\")\n# Plot side-by-side boxplots\nsns.boxplot(data = all_shap_base, x ='ADMISSION_TYPE_ELECTIVE', y = 'ADMISSION_TYPE_ELECTIVE_explanation',\n            hue='ADMISSION_TYPE_ELECTIVE', width=0.8, ax=axes[0])\naxes[0].set_title('Base Model')\nsns.boxplot(data = all_shap_opt, x ='ADMISSION_TYPE_ELECTIVE', y = 'ADMISSION_TYPE_ELECTIVE_explanation',\n            hue='ADMISSION_TYPE_ELECTIVE', width=0.4, ax = axes[1])\naxes[1].set_title('Opt Model')\n\n# Customize and show the plot\nfor ax in axes:\n    ax.set_xlabel(\"Elective Admission (1 = True)\")\n    ax.set_ylabel(\"Influence\")\n    ax.legend(loc='upper right')\n\nplt.show()"
    },
    {
      "cell_type": "markdown",
      "id": "df7a9ccc-e785-4a82-b9e9-97fd44d5acf2",
      "metadata": {
        "collapsed": false,
        "name": "Monitoring_section",
        "resultHeight": 74,
        "codeCollapsed": true
      },
      "source": "# Model Monitoring setup"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e0751bdd-6c24-4c65-9247-aa90ebc1d376",
      "metadata": {
        "collapsed": false,
        "language": "python",
        "name": "create_table_from_test_data",
        "resultHeight": 0
      },
      "outputs": [],
      "source": "train.write.save_as_table(f\"DEMO_PATIENT_READMISSION_TRAIN_{VERSION_NUM}\", mode=\"overwrite\")\ntest.write.save_as_table(f\"DEMO_PATIENT_READMISSION_TEST_{VERSION_NUM}\", mode=\"overwrite\")"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aabdf2be-87f8-4556-aa42-22e4a70515e1",
      "metadata": {
        "codeCollapsed": false,
        "collapsed": false,
        "language": "python",
        "name": "create_stage",
        "resultHeight": 111
      },
      "outputs": [],
      "source": "session.sql(\"CREATE stage IF NOT EXISTS ML_STAGE\").collect()"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "21b2c090-5cc8-4847-982a-fb9b5e427616",
      "metadata": {
        "codeCollapsed": false,
        "collapsed": false,
        "language": "python",
        "name": "define_sproc",
        "resultHeight": 495
      },
      "outputs": [],
      "source": "from snowflake import snowpark\n\ndef demo_inference_sproc(session: snowpark.Session, table_name: str, modelname: str, modelversion: str) -> str:\n\n    reg = Registry(session=session)\n    m = reg.get_model(model_name)  # Fetch the model using the registry\n    mv = m.version(modelversion)\n    \n    input_table_name=table_name\n    pred_col = f'{modelversion}_PREDICTION'\n\n    # Read the input table to a dataframe\n    df = session.table(input_table_name)\n    results = mv.run(df, function_name=\"predict\").select(\"PATIENT_ID\",'\"output_feature_0\"').withColumnRenamed('\"output_feature_0\"', pred_col)\n    # 'results' is the output DataFrame with predictions\n\n    final = df.join(results, on=\"PATIENT_ID\", how=\"full\")\n    # Write results back to Snowflake table\n    final.write.save_as_table(table_name, mode='overwrite',enable_schema_evolution=True)\n\n    return \"Success\"\n\n# Register the stored procedure\nsession.sproc.register(\n    func=demo_inference_sproc,\n    name=\"model_inference_sproc\",\n    replace=True,\n    is_permanent=True,\n    stage_location=\"@ML_STAGE\",\n    packages=['joblib', 'snowflake-snowpark-python', 'snowflake-ml-python'],\n    return_type=StringType()\n)"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "da45031a-917e-4f6d-a2e4-068879791819",
      "metadata": {
        "codeCollapsed": false,
        "collapsed": false,
        "language": "sql",
        "name": "gb_base_train_inference",
        "resultHeight": 111,
        "resultVariableName": "dataframe_1"
      },
      "outputs": [],
      "source": "CALL model_inference_sproc('DEMO_PATIENT_READMISSION_TRAIN_{{VERSION_NUM}}','{{model_name}}', '{{base_version_name}}');"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0d18ea05-7d29-43a3-9baa-52509f3bb15e",
      "metadata": {
        "codeCollapsed": false,
        "collapsed": false,
        "language": "sql",
        "name": "gb_base_test_inference",
        "resultHeight": 111,
        "resultVariableName": "dataframe_2"
      },
      "outputs": [],
      "source": "CALL model_inference_sproc('DEMO_PATIENT_READMISSION_TEST_{{VERSION_NUM}}','{{model_name}}', '{{base_version_name}}');"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c1d2550b-46c7-4eb7-adaa-64c345711b1e",
      "metadata": {
        "codeCollapsed": false,
        "collapsed": false,
        "language": "sql",
        "name": "gb_opt_train_inference",
        "resultHeight": 111,
        "resultVariableName": "dataframe_3"
      },
      "outputs": [],
      "source": "CALL model_inference_sproc('DEMO_PATIENT_READMISSION_TRAIN_{{VERSION_NUM}}','{{model_name}}', '{{optimized_version_name}}');"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8245f482-19e9-4961-9cb2-801bf5948d52",
      "metadata": {
        "codeCollapsed": false,
        "collapsed": false,
        "language": "sql",
        "name": "gb_opt_test_inference",
        "resultHeight": 111,
        "resultVariableName": "dataframe_4"
      },
      "outputs": [],
      "source": "CALL model_inference_sproc('DEMO_PATIENT_READMISSION_TEST_{{VERSION_NUM}}','{{model_name}}', '{{optimized_version_name}}');"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ec05048c-a9d1-4ef9-bf39-5333f3fb56cb",
      "metadata": {
        "codeCollapsed": false,
        "collapsed": false,
        "language": "sql",
        "name": "see_preds",
        "resultHeight": 251,
        "resultVariableName": "dataframe_5"
      },
      "outputs": [],
      "source": "select TIMESTAMP, PATIENT_ID, PATIENT_AGE, TREATMENT_COST, XGB_BASE_PREDICTION, XGB_OPTIMIZED_PREDICTION, READMITTED \nFROM DEMO_PATIENT_READMISSION_TEST_{{VERSION_NUM}} \nlimit 20"
    },
    {
      "cell_type": "markdown",
      "id": "416c348e-0e6d-4ee3-88c3-59077b409621",
      "metadata": {
        "name": "model_monitor_markdown",
        "collapsed": false,
        "codeCollapsed": true
      },
      "source": "## Now that our models have been deployed and we have run inference - lets set up ML Observability!\n\n- First we will add a column to our inference data to later explore with our segmentation capabilities \n- We will define a model monitor for each model, with the training data as our baseline and the test data representing inference results. \n- Once the monitors are defined we can access them via the Model Registry \n    - We can also query drift metrics etc. programmatically"
    },
    {
      "cell_type": "code",
      "id": "a16befd0-410b-4777-bbf6-bfa3580c4973",
      "metadata": {
        "language": "sql",
        "name": "create_segment_col_test",
        "resultVariableName": "dataframe_6"
      },
      "outputs": [],
      "source": "ALTER TABLE DEMO_PATIENT_READMISSION_TEST_{{VERSION_NUM}}\nADD COLUMN IF NOT EXISTS ADMISSION_CATEGORY VARCHAR(50);\n\n\nUPDATE DEMO_PATIENT_READMISSION_TEST_{{VERSION_NUM}}\nSET ADMISSION_CATEGORY = CASE\n    WHEN ADMISSION_TYPE_EMERGENCY = 1 THEN 'EMERGENCY'\n    WHEN ADMISSION_TYPE_ELECTIVE = 1 THEN 'ELECTIVE'\n    WHEN ADMISSION_TYPE_URGENT = 1 THEN 'URGENT'\n    ELSE 'OTHER'\nEND;",
      "execution_count": null
    },
    {
      "cell_type": "code",
      "id": "f5a191e3-6311-4bd2-a613-5b21a1df9082",
      "metadata": {
        "language": "sql",
        "name": "create_segment_col_train",
        "resultVariableName": "dataframe_7"
      },
      "outputs": [],
      "source": "ALTER TABLE DEMO_PATIENT_READMISSION_TRAIN_{{VERSION_NUM}}\nADD COLUMN IF NOT EXISTS ADMISSION_CATEGORY VARCHAR(50);\n\n\nUPDATE DEMO_PATIENT_READMISSION_TRAIN_{{VERSION_NUM}}\nSET ADMISSION_CATEGORY = CASE\n    WHEN ADMISSION_TYPE_EMERGENCY = 1 THEN 'EMERGENCY'\n    WHEN ADMISSION_TYPE_ELECTIVE = 1 THEN 'ELECTIVE'\n    WHEN ADMISSION_TYPE_URGENT = 1 THEN 'URGENT'\n    ELSE 'OTHER'\nEND;",
      "execution_count": null
    },
    {
      "cell_type": "code",
      "id": "c8d51f7b-8e3e-4a30-b7c5-606986f668cd",
      "metadata": {
        "language": "sql",
        "name": "view_loan_purpose_data",
        "resultVariableName": "dataframe_8"
      },
      "outputs": [],
      "source": "SELECT ADMISSION_TYPE_EMERGENCY, ADMISSION_TYPE_ELECTIVE, ADMISSION_TYPE_URGENT, ADMISSION_CATEGORY FROM DEMO_PATIENT_READMISSION_TEST_{{VERSION_NUM}} limit 10;",
      "execution_count": null
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2f6be548-47cb-4a91-92ee-a5f42c41e756",
      "metadata": {
        "codeCollapsed": false,
        "collapsed": false,
        "language": "sql",
        "name": "create_model_monitor_base",
        "resultHeight": 111,
        "resultVariableName": "dataframe_9"
      },
      "outputs": [],
      "source": "CREATE OR REPLACE MODEL MONITOR PATIENT_READMISSION_BASE_MODEL_MONITOR\nWITH\n    MODEL={{model_name}}\n    VERSION={{base_version_name}}\n    FUNCTION=predict\n    SOURCE=DEMO_PATIENT_READMISSION_TEST_{{VERSION_NUM}}\n    BASELINE=DEMO_PATIENT_READMISSION_TRAIN_{{VERSION_NUM}}\n    TIMESTAMP_COLUMN=TIMESTAMP\n    PREDICTION_CLASS_COLUMNS=(XGB_BASE_PREDICTION)  \n    ACTUAL_CLASS_COLUMNS=(READMITTED)\n    ID_COLUMNS=(PATIENT_ID)\n    SEGMENT_COLUMNS = ('ADMISSION_CATEGORY')\n    WAREHOUSE={{COMPUTE_WAREHOUSE}}\n    REFRESH_INTERVAL='12 hours'\n    AGGREGATION_WINDOW='1 day';"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "60965976-f17f-42bc-92ae-e43030bba54e",
      "metadata": {
        "codeCollapsed": false,
        "collapsed": false,
        "language": "sql",
        "name": "create_model_monitor_optimized",
        "resultHeight": 111,
        "resultVariableName": "dataframe_10"
      },
      "outputs": [],
      "source": "CREATE OR REPLACE MODEL MONITOR PATIENT_READMISSION_OPTIMIZED_MODEL_MONITOR\nWITH\n    MODEL={{model_name}}\n    VERSION={{optimized_version_name}}\n    FUNCTION=predict\n    SOURCE=DEMO_PATIENT_READMISSION_TEST_{{VERSION_NUM}}\n    BASELINE=DEMO_PATIENT_READMISSION_TRAIN_{{VERSION_NUM}}\n    TIMESTAMP_COLUMN=TIMESTAMP\n    PREDICTION_CLASS_COLUMNS=(XGB_OPTIMIZED_PREDICTION)  \n    ACTUAL_CLASS_COLUMNS=(READMITTED)\n    ID_COLUMNS=(PATIENT_ID)\n    SEGMENT_COLUMNS = ('ADMISSION_CATEGORY')\n    WAREHOUSE={{COMPUTE_WAREHOUSE}}\n    REFRESH_INTERVAL='12 hours'\n    AGGREGATION_WINDOW='1 day';"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "537fc658-38f4-4c9a-980b-cf40ef61a268",
      "metadata": {
        "codeCollapsed": false,
        "language": "sql",
        "name": "compute_prediction_drift",
        "resultVariableName": "dataframe_11"
      },
      "outputs": [],
      "source": "SELECT * FROM TABLE(MODEL_MONITOR_DRIFT_METRIC(\n'PATIENT_READMISSION_BASE_MODEL_MONITOR', -- model monitor to use\n'DIFFERENCE_OF_MEANS', -- metric for computing drift\n'XGB_BASE_PREDICTION', -- column to compute drift on\n'1 DAY',  -- day granularity for drift computation\nDATEADD(DAY, -90, CURRENT_DATE()), -- end date\nDATEADD(DAY, -60, CURRENT_DATE()) -- start date\n)\n)"
    },
    {
      "id": "25dc336c-3df6-47f3-bef7-66f5148b587c",
      "cell_type": "markdown",
      "metadata": {
        "codeCollapsed": true
      },
      "source": "# Retraining pipeline"
    },
    {
      "id": "bf9c5880-78ac-4026-8eb9-aa06922e4a35",
      "cell_type": "code",
      "metadata": {
        "resultVariableName": "dataframe_14",
        "language": "sql"
      },
      "source": "-- Create table to store daily drift metrics\nCREATE OR REPLACE TABLE PATIENT_READMISSION_DRIFT_METRICS (\n    METRIC_DATE DATE,\n    MODEL_MONITOR_NAME VARCHAR(200),\n    COLUMN_NAME VARCHAR(200),\n    DRIFT_METRIC_NAME VARCHAR(100),\n    DRIFT_VALUE FLOAT,\n    THRESHOLD_EXCEEDED BOOLEAN,\n    RECORDED_AT TIMESTAMP_NTZ DEFAULT CURRENT_TIMESTAMP()\n);",
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "ba2a132e-b603-4f5c-a4f3-cdf990303e43",
      "cell_type": "code",
      "metadata": {
        "resultVariableName": "dataframe_15",
        "language": "sql"
      },
      "source": "-- Create task to compute and store drift metrics daily\nCREATE OR REPLACE TASK DAILY_DRIFT_METRICS_UPDATE\n  WAREHOUSE = {{COMPUTE_WAREHOUSE}}\n  SCHEDULE = 'USING CRON 0 2 * * * America/Los_Angeles'  -- 2 AM daily\nAS\nINSERT INTO PATIENT_READMISSION_DRIFT_METRICS (\n    METRIC_DATE,\n    MODEL_MONITOR_NAME,\n    COLUMN_NAME,\n    DRIFT_METRIC_NAME,\n    DRIFT_VALUE,\n    THRESHOLD_EXCEEDED\n)\nSELECT \n    CURRENT_DATE() AS METRIC_DATE,\n    'PATIENT_READMISSION_BASE_MODEL_MONITOR' AS MODEL_MONITOR_NAME,\n    'XGB_BASE_PREDICTION' AS COLUMN_NAME,\n    'DIFFERENCE_OF_MEANS' AS DRIFT_METRIC_NAME,\n    dm.METRIC_VALUE,\n    CASE \n        WHEN ABS(dm.METRIC_VALUE) > 0.05 THEN TRUE  -- 5% threshold\n        ELSE FALSE \n    END AS THRESHOLD_EXCEEDED\nFROM TABLE(MODEL_MONITOR_DRIFT_METRIC(\n    'PATIENT_READMISSION_BASE_MODEL_MONITOR',\n    'DIFFERENCE_OF_MEANS',\n    'XGB_BASE_PREDICTION',\n    '1 DAY',\n    DATEADD(DAY, -1, CURRENT_DATE()),\n    CURRENT_DATE()\n)) dm;",
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "d4808430-f5bc-45ae-98a3-98ebf5e6454d",
      "cell_type": "code",
      "metadata": {
        "resultVariableName": "dataframe_16",
        "language": "sql"
      },
      "source": "-- Create stream to track changes to drift metrics table\nCREATE OR REPLACE STREAM DRIFT_METRICS_STREAM\n  ON TABLE PATIENT_READMISSION_DRIFT_METRICS\n  APPEND_ONLY = TRUE;",
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "7876c51f-3bf8-4b0a-8493-07afb726bee1",
      "cell_type": "code",
      "metadata": {
        "language": "python"
      },
      "source": "from snowflake.snowpark.types import StringType\n\ndef trigger_hpo_training_job(session: snowpark.Session) -> str:\n    \"\"\"\n    Trigger HPO training using Snowpark ML Jobs API\n    \"\"\"\n    import json\n    from datetime import datetime\n    from snowflake.ml.jobs import remote\n    \n    job_id = f\"hpo_retrain_{datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n    \n    \n    # Ensure compute pool exists\n    try:\n        session.sql(\"\"\"\n            CREATE COMPUTE POOL IF NOT EXISTS ML_HPO_COMPUTE_POOL\n            MIN_NODES = 1\n            MAX_NODES = 3\n            INSTANCE_FAMILY = CPU_X64_M\n            AUTO_RESUME = TRUE\n            AUTO_SUSPEND_SECS = 300\n        \"\"\").collect()\n        print(\"Compute pool created/verified\")\n    except Exception as e:\n        print(f\"Compute pool setup: {e}\")\n\n    @remote(\n        compute_pool='ML_HPO_COMPUTE_POOL', \n        stage_name='ML_STAGE',\n        pip_requirements=[\n            'snowflake-snowpark-python',\n            'snowflake-ml-python',\n            'xgboost',\n            'scikit-learn',\n            'pandas'\n        ],\n        python_version='3.12'\n    )\n    def hpo_training_job():\n        \"\"\"\n        ML Job function to perform HPO training and register new model version\n        This will run on Snowpark Container Services\n        \"\"\"\n        from snowflake.snowpark import Session\n        from snowflake.ml.registry import Registry\n        from snowflake.ml.modeling.tune import get_tuner_context\n        from snowflake.ml.modeling import tune\n        from snowflake.ml.data import DataConnector\n        from xgboost import XGBClassifier\n        from sklearn.metrics import f1_score, precision_score, recall_score\n        import pandas as pd\n        from datetime import datetime\n        \n        # Get session (will be automatically provided in ML Job context)\n        session = Session.builder.getOrCreate()\n        \n        # Configuration\n        VERSION_NUM = '0'\n        DB = \"E2E_SNOW_MLOPS_DB\"\n        SCHEMA = \"MLOPS_SCHEMA\"\n        \n        # Get training data\n        train_table = f\"DEMO_PATIENT_READMISSION_TRAIN_{VERSION_NUM}\"\n        test_table = f\"DEMO_PATIENT_READMISSION_TEST_{VERSION_NUM}\"\n        \n        train_df = session.table(train_table)\n        test_df = session.table(test_table)\n        \n        # Define dataset map\n        dataset_map = {\n            \"x_train\": DataConnector.from_dataframe(\n                train_df.drop(\"READMITTED\", \"TIMESTAMP\", \"PATIENT_ID\", \"ADMISSION_CATEGORY\")\n            ),\n            \"y_train\": DataConnector.from_dataframe(train_df.select(\"READMITTED\")),\n            \"x_test\": DataConnector.from_dataframe(\n                test_df.drop(\"READMITTED\", \"TIMESTAMP\", \"PATIENT_ID\", \"ADMISSION_CATEGORY\")\n            ),\n            \"y_test\": DataConnector.from_dataframe(test_df.select(\"READMITTED\"))\n        }\n        \n        # Define training function for HPO\n        def train_func():\n            tuner_context = get_tuner_context()\n            config = tuner_context.get_hyper_params()\n            dm = tuner_context.get_dataset_map()\n            \n            model = XGBClassifier(**config, random_state=42)\n            \n            X_train_pd = dm[\"x_train\"].to_pandas().sort_index()\n            y_train_pd = dm[\"y_train\"].to_pandas().sort_index()\n            X_test_pd = dm[\"x_test\"].to_pandas().sort_index()\n            y_test_pd = dm[\"y_test\"].to_pandas().sort_index()\n            \n            model.fit(X_train_pd, y_train_pd)\n            \n            train_preds = model.predict(X_train_pd)\n            test_preds = model.predict(X_test_pd)\n            \n            f1_train = f1_score(y_train_pd, train_preds)\n            precision_train = precision_score(y_train_pd, train_preds)\n            recall_train = recall_score(y_train_pd, train_preds)\n            \n            f1_test = f1_score(y_test_pd, test_preds)\n            precision_test = precision_score(y_test_pd, test_preds)\n            recall_test = recall_score(y_test_pd, test_preds)\n            \n            tuner_context.report(\n                F1_Train=f1_train,\n                Precision_Train=precision_train,\n                Recall_Train=recall_train,\n                F1_Test=f1_test,\n                Precision_Test=precision_test,\n                Recall_Test=recall_test\n            )\n            \n            return model\n        \n        # Configure and run HPO tuner\n        tuner = tune.Tuner(\n            trainable=train_func,\n            param_space={\n                \"max_depth\": tune.randint(3, 10),\n                \"learning_rate\": tune.uniform(0.01, 0.3),\n                \"n_estimators\": tune.randint(50, 200)\n            },\n            tune_config=tune.TuneConfig(\n                metric=\"F1_Test\",\n                mode=\"max\",\n                num_samples=8\n            )\n        )\n        \n        print(\"Starting HPO tuning...\")\n        tuner_results = tuner.run(dataset_map=dataset_map)\n        best_model = tuner_results.best_model\n        \n        # Get best metrics\n        best_config = tuner_results.best_config\n        best_result = tuner_results.results.loc[tuner_results.results['F1_Test'].idxmax()]\n        \n        print(f\"Best config: {best_config}\")\n        print(f\"Best F1 Test: {best_result['F1_Test']}\")\n        \n        # Register model with version suffix as current date\n        model_name = f\"PATIENT_READMISSION_MLOPS_{VERSION_NUM}\"\n        version_suffix = datetime.now().strftime('%Y%m%d_%H%M%S')\n        new_version = f\"XGB_RETRAINED_{version_suffix}\"\n        \n        registry = Registry(\n            session=session,\n            database_name=DB,\n            schema_name=SCHEMA,\n            options={\"enable_monitoring\": True}\n        )\n        \n        print(f\"Registering model version: {new_version}\")\n        mv = registry.log_model(\n            model_name=model_name,\n            model=best_model,\n            version_name=new_version,\n            sample_input_data=train_df.drop(\n                [\"TIMESTAMP\", \"PATIENT_ID\", \"READMITTED\", \"ADMISSION_CATEGORY\"]\n            ).limit(100),\n            comment=f\"\"\"Automatically retrained model due to drift detection.\n                       HPO optimized with max_depth={best_config['max_depth']},\n                       learning_rate={best_config['learning_rate']:.4f},\n                       n_estimators={best_config['n_estimators']}\n                       Triggered by drift monitoring on {version_suffix}\"\"\",\n            target_platforms=[\"WAREHOUSE\", \"SNOWPARK_CONTAINER_SERVICES\"],\n            options={\"enable_explainability\": True}\n        )\n        \n        # Log metrics\n        mv.set_metric(metric_name=\"Train_F1_Score\", value=float(best_result['F1_Train']))\n        mv.set_metric(metric_name=\"Train_Precision_Score\", value=float(best_result['Precision_Train']))\n        mv.set_metric(metric_name=\"Train_Recall_score\", value=float(best_result['Recall_Train']))\n        mv.set_metric(metric_name=\"Test_F1_Score\", value=float(best_result['F1_Test']))\n        mv.set_metric(metric_name=\"Test_Precision_Score\", value=float(best_result['Precision_Test']))\n        mv.set_metric(metric_name=\"Test_Recall_score\", value=float(best_result['Recall_Test']))\n        \n        print(f\"Successfully trained and registered model version: {new_version}\")\n        \n        return {\n            \"model_version\": new_version,\n            \"f1_score\": float(best_result['F1_Test']),\n            \"status\": \"success\"\n        }\n        \n    try:\n        # Submit the job\n        result = hpo_training_job()\n        return f\"HPO training job {job_id} submitted: {result}\"\n    except Exception as e:\n        return f\"Error submitting job: {str(e)}\"\n\n# Register the stored procedure\nsession.sproc.register(\n    func=trigger_hpo_training_job,\n    name=\"TRIGGER_HPO_TRAINING_JOB\",\n    replace=True,\n    is_permanent=True,\n    stage_location=\"@ML_STAGE\",\n    packages=['snowflake-snowpark-python','snowflake-ml-python'],\n    return_type=StringType(),\n    python_version=\"3.12\"\n)\n\nprint(\"HPO training job trigger registered\")",
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "4ec8da1b-bffe-4ef4-9ec5-80ee16f1dfb6",
      "cell_type": "code",
      "metadata": {
        "resultVariableName": "dataframe_17",
        "language": "sql"
      },
      "source": "-- Create task that triggers HPO training job when drift detected\nCREATE OR REPLACE TASK RETRAIN_ON_DRIFT_DETECTION\n  WAREHOUSE = {{COMPUTE_WAREHOUSE}}\n  AFTER DAILY_DRIFT_METRICS_UPDATE\nWHEN\n  SYSTEM$STREAM_HAS_DATA('DRIFT_METRICS_STREAM')\nAS\nDECLARE\n  drift_detected BOOLEAN;\n  job_result VARCHAR;\nBEGIN\n  -- Check if drift threshold exceeded\n  SELECT MAX(THRESHOLD_EXCEEDED)::BOOLEAN\n  INTO :drift_detected\n  FROM DRIFT_METRICS_STREAM\n  WHERE METRIC_DATE >= DATEADD(DAY, -1, CURRENT_DATE());\n  \n  -- Trigger HPO training job if drift detected\n  IF (:drift_detected = TRUE) THEN\n    CALL TRIGGER_HPO_TRAINING_JOB() INTO :job_result;\n    \n    -- Log the event\n    INSERT INTO PATIENT_READMISSION_DRIFT_METRICS (\n      METRIC_DATE,\n      MODEL_MONITOR_NAME,\n      COLUMN_NAME,\n      DRIFT_METRIC_NAME,\n      DRIFT_VALUE,\n      THRESHOLD_EXCEEDED\n    )\n    VALUES (\n      CURRENT_DATE(),\n      'RETRAINING_EVENT',\n      'HPO_JOB_ID',\n      :job_result,\n      NULL,\n      TRUE\n    );\n  END IF;\nEND;",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "id": "7b8f8d88-1ebe-4622-89ca-39bce08473d4",
      "metadata": {
        "collapsed": false,
        "name": "SPCS_MD",
        "codeCollapsed": true
      },
      "source": "# SPCS Deployment setup (OPTIONAL)\n## This is disabled by default but uncommenting the below code cells will allow a user to \n\n- ### Create a new compute pool with 3 XL CPU nodes\n- ### Deploys a service on top of our existing HPO model version\n- ### Tests out inference on newly created container service"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c416a4d0-a95f-4702-9a61-26b61706eb11",
      "metadata": {
        "collapsed": false,
        "language": "python",
        "name": "define_spcs_vars"
      },
      "outputs": [],
      "source": "cp_name = \"PATIENT_READMISSION_INFERENCE_CP\"\nnum_spcs_nodes = '2'\nspcs_instance_family = 'CPU_X64_L'\nservice_name = 'PATIENT_READMISSION_PREDICTION_SERVICE'\n\ncurrent_database = session.get_current_database().replace('\"', '')\ncurrent_schema = session.get_current_schema().replace('\"', '')\nextended_service_name = f'{current_database}.{current_schema}.{service_name}'"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "448f6702-8fb2-4aac-9e54-a8673c064074",
      "metadata": {
        "codeCollapsed": false,
        "language": "python",
        "name": "setup_compute_pool"
      },
      "outputs": [],
      "source": "session.sql(f\"alter compute pool if exists {cp_name} stop all\").collect()\nsession.sql(f\"drop compute pool if exists {cp_name}\").collect()\nsession.sql(f\"create compute pool {cp_name} min_nodes={num_spcs_nodes} max_nodes={num_spcs_nodes} instance_family={spcs_instance_family} auto_resume=True auto_suspend_secs=300\").collect()\nsession.sql(f\"describe compute pool {cp_name}\").show()"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "df47725b-e9e7-4f93-bdea-9db09794bd95",
      "metadata": {
        "codeCollapsed": false,
        "collapsed": false,
        "language": "python",
        "name": "spcs_deploy_service"
      },
      "outputs": [],
      "source": "#note this may take up to 5 minutes to run\n\nmv_opt.create_service(\n    service_name=extended_service_name,\n    service_compute_pool=cp_name,\n    ingress_enabled=True,\n    max_instances=int(num_spcs_nodes)\n)"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "25c4f6d4-b16b-4448-bcf3-4f128ccfbe43",
      "metadata": {
        "codeCollapsed": false,
        "language": "python",
        "name": "see_model_versions_with_services"
      },
      "outputs": [],
      "source": "model_registry.get_model(model_name).show_versions()"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c5eb9b1a-0741-4e9d-aaf4-2da26c44ffbd",
      "metadata": {
        "codeCollapsed": false,
        "collapsed": false,
        "language": "python",
        "name": "run_SPCS_inference"
      },
      "outputs": [],
      "source": "mv_container = model_registry.get_model(model_name).default\nmv_container.run(test, function_name = \"predict\", service_name = \"PATIENT_READMISSION_PREDICTION_SERVICE\").rename('\"output_feature_0\"', 'XGB_PREDICTION')"
    },
    {
      "cell_type": "code",
      "id": "b4e97fa5-d50c-40c8-a5a9-6135a4162adf",
      "metadata": {
        "language": "sql",
        "name": "view_endpoints",
        "resultVariableName": "dataframe_12"
      },
      "outputs": [],
      "source": "SHOW ENDPOINTS IN SERVICE E2E_SNOW_MLOPS_DB.MLOPS_SCHEMA.PATIENT_READMISSION_PREDICTION_SERVICE",
      "execution_count": null
    },
    {
      "cell_type": "code",
      "id": "8fdea708-d656-41b7-92d8-45eca76ccf70",
      "metadata": {
        "language": "sql",
        "name": "cell1",
        "resultVariableName": "dataframe_13"
      },
      "outputs": [],
      "source": "SHOW MODELS;",
      "execution_count": null
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "35388bca-f70f-47db-a3ef-3558dda91502",
      "metadata": {
        "codeCollapsed": false,
        "language": "python",
        "name": "stop_compute_pool"
      },
      "outputs": [],
      "source": "#Stop the service to save costs\n#session.sql(f\"alter compute pool if exists {cp_name} stop all\").collect()"
    },
    {
      "cell_type": "markdown",
      "id": "ce110000-1111-2222-3333-ffffff000036",
      "metadata": {
        "collapsed": false,
        "jp-MarkdownHeadingCollapsed": true,
        "name": "conclusion",
        "resultHeight": 202,
        "tags": [],
        "codeCollapsed": true
      },
      "source": "## Conclusion \n\n#### 🛠️ Snowflake Feature Store tracks feature definitions and maintains lineage of sources and destinations 🛠️\n#### 🚀 Snowflake Model Registry gives users a secure and flexible framework to log models, tag candidates for production, and run inference and explainability jobs 🚀\n#### 📈 ML observability in Snowflake allows users to monitor model performance over time and detect model, feature, and concept drift 📈\n#### 🔮 All models logged in the Model Registry can be accessed for inference, explainability, lineage tracking, visibility and more 🔮"
    }
  ]
}